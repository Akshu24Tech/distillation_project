# ğŸ§  Knowledge Distillation from Neural Networks

A PyTorch implementation of the seminal research paper:

> **Distilling the Knowledge in a Neural Network**  
> Geoffrey Hinton, Oriol Vinyals, Jeff Dean â€” [arXiv:1503.02531](https://arxiv.org/abs/1503.02531)

This project demonstrates how to transfer knowledge from a large, high-performing **Teacher model** to a smaller, efficient **Student model** using **soft targets** and **temperature-scaled outputs**.

---

## ğŸ“˜ What is Knowledge Distillation?

Knowledge distillation is a compression technique where a small model (student) is trained to replicate the behavior of a large model (teacher). Rather than training the student on hard labels (like `[0, 0, 1]`), it learns from the **soft probability distributions** output by the teacher. This allows the student to mimic the generalization capability of the teacher while being faster and lighter.

---

## ğŸ”¬ Research Paper Summary

- Teacher: Large network trained with ground truth
- Student: Smaller model trained using:
  - Hard labels (`CrossEntropy`)
  - Soft targets from the teacher (`KL Divergence`)
- Temperature `T` is used to soften the teacher outputs:
  \[
  \text{Loss} = \alpha \cdot \text{CE} + \beta \cdot T^2 \cdot \text{KL}(\text{soft}_T^{\text{teacher}} || \text{soft}_T^{\text{student}})
  \]

---

## ğŸ“‚ Project Structure

```

distillation\_project/
â”‚
â”œâ”€â”€ models/                   # Teacher and Student architectures
â”‚   â””â”€â”€ student.py
â”‚
â”œâ”€â”€ utils/                    # Training and evaluation scripts
â”‚   â”œâ”€â”€ train\_teacher.py
â”‚   â”œâ”€â”€ generate\_soft\_labels.py
â”‚   â””â”€â”€ evaluate.py
â”‚
â”œâ”€â”€ outputs/                  # Model weights and soft label dumps
â”‚   â”œâ”€â”€ teacher\_resnet50.pth
â”‚   â”œâ”€â”€ student\_cnn.pth
â”‚   â””â”€â”€ soft\_labels.npy
â”‚
â”œâ”€â”€ distill.py                # Main distillation training script
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

````

---

## ğŸš€ How to Run

### 1. ğŸ”§ Install Requirements

```bash
pip install -r requirements.txt
````

### 2. ğŸ‹ï¸â€â™‚ï¸ Train the Teacher Model (ResNet-50 on CIFAR-10)

```bash
python utils/train_teacher.py
```

### 3. ğŸ”¥ Generate Soft Labels using the Teacher

```bash
python utils/generate_soft_labels.py
```

### 4. ğŸ‘¨â€ğŸ« Train the Student Model with Distillation

```bash
python distill.py
```

### 5. ğŸ“Š Evaluate Both Models

```bash
python utils/evaluate.py
```

---

## ğŸ“ˆ Results

| Model                | Accuracy (%) | Parameters | Size (MB) |
| -------------------- | ------------ | ---------- | --------- |
| Teacher (ResNet-50)  | \~93%        | 23M        | \~98 MB   |
| Student (Custom CNN) | \~88%        | <1M        | \~3 MB    |

> âœ… Significant reduction in size with minimal accuracy drop.

---

## ğŸ§ª Datasets

* CIFAR-10 â€” 60,000 32x32 color images in 10 classes
* Automatically downloaded via `torchvision.datasets.CIFAR10`

---

## ğŸ“Œ Techniques Used

* Knowledge Distillation
* Soft Targets with Temperature Scaling
* KL Divergence
* PyTorch + torchvision
* Model compression

---

## ğŸ“š References

* [Distilling the Knowledge in a Neural Network (arXiv:1503.02531)](https://arxiv.org/abs/1503.02531)
* Hinton et al., 2015
* PyTorch Tutorials

---

## ğŸ› ï¸ Future Work

* Replace Student CNN with ResNet-18 or MobileNet
* Visualize soft targets and class probabilities
* Apply to other datasets (e.g., ImageNet, TinyImageNet)
* Deploy with ONNX or TensorRT for inference

---

## ğŸ™Œ Acknowledgements

Built as part of an educational implementation of classic AI research.
Thanks to [Geoffrey Hinton](https://scholar.google.com/citations?user=JicYPdAAAAAJ&hl=en) and the original authors for their foundational work.

---

## ğŸ“Œ License

This project is open-source for educational and research purposes.
